{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*****************Filtered****************************\n",
      "                                                    0\n",
      "0   RT @ VijayaRahatkar : 'Women Loves Challenges....\n",
      "1   RT @ CNN : Pakistan Prime Minister Imran Khan ...\n",
      "2   Jio-Microsoft Cloud Tie-Up Poses Threat Amazon...\n",
      "3   RT @ davidfrawleyved : Pakistan defined nation...\n",
      "4   RT @ PiyushGoyal : Delighted address plenary '...\n",
      "5   RT @ SuryahSG : 1965 India - Pakistan War ! Ac...\n",
      "6   RT @ AkhileshPSingh : ARTICLE 370 अब @ BJP4Ind...\n",
      "7   RT @ PiyushGoyal : Railways manufactured high ...\n",
      "8   RT @ CNN : Indian actress Priyanka Chopra crit...\n",
      "9   RT @ CNN : Indian actress Priyanka Chopra crit...\n",
      "10  RT @ BBCHindi : अमरिंदर सिंह ने यूं जीता कश्मी...\n",
      "11  RT @ iAbheejit : Twitter magic ! I 'm looking ...\n",
      "12  RT @ AmarUjalaNews : कश्मीर में तैनात इन दो मह...\n",
      "13  RT @ ShashiTharoor : Interesting piece @ saket...\n",
      "14  RT @ Puneetvizh : RBI ’ unusual rate cut adds ...\n",
      "15  RT @ nadhirthegreat : Interpol even rejected I...\n",
      "16  RT @ PiyushGoyal : Railways manufactured high ...\n",
      "17  RT @ madhukishwar : 2 . Yet Hindu hating Musli...\n",
      "18  Everything I want say said I 'm . LOL literall...\n",
      "19  If question Kashmiri journalist Pakistan done ...\n",
      "20  RT @ CNNnews18 : # Article370Revoked | People ...\n",
      "21  RT @ BilalTa49287662 : I always stand kashmir ...\n",
      "22  Well , I guess , confidence comes big ass chan...\n",
      "23  RT @ AsraNomani : FYI @ priyankachopra “ war m...\n",
      "24  RT @ RavinarIN : Not forget ... Criminal `` Re...\n",
      "25  RT @ Pramila61709169 : 46 % Rape cases Found F...\n",
      "26  Jio-Microsoft Cloud Tie-Up Poses Threat Amazon...\n",
      "27  RT @ thatdoggonelady : Freelancers India , goi...\n",
      "28  RT @ vinayak_jain : Saudi Arabia invest 20 % s...\n",
      "29  RT @ ShefVaidya : Question asking , Abdul Basi...\n",
      "..                                                ...\n",
      "70  RT @ AlArabiya_Eng : Pakistani Prime Minister ...\n",
      "71  RT @ Jhagra : 1 . India , country 200 mln Musl...\n",
      "72  RT @ AMBAbsolute : @ poojabeditweets @ smritii...\n",
      "73  RT @ trtworld : Muslims India-administered Kas...\n",
      "74  RT @ CNN : Pakistan Prime Minister Imran Khan ...\n",
      "75  RT @ InventionsVedic : @ SerbianPM @ realDonal...\n",
      "76  RT @ ZeeNewsHindi : सुषमा स्वराज को याद कर रहा...\n",
      "77  've seen pakistani trolls n't behave like ever...\n",
      "78  RT @ nytimes : @ sameeryasir @ suhasiniraj The...\n",
      "79  RT @ amritabhinder : Everyone world backing In...\n",
      "80  RT @ AMIT_GUJJU : First Adhir Ranjan said Kash...\n",
      "81  RT @ AdityaRajKaul : The magical moment flew p...\n",
      "82  Hey Shobha De , Stop sending terrorist Pakista...\n",
      "83  RT @ CMANN66 : More pollution India https : //...\n",
      "84  RT @ ANI : # WATCH Columnist Shobhaa De respon...\n",
      "85  RT @ SharmaAlok_ : Lynching become common Indi...\n",
      "86  RT @ indiantweeter : Why would Abdul basit lie...\n",
      "87  RT @ c_aashish : Illegal Cow Smuggling From In...\n",
      "88  RT @ rammadhavbjp : An interesting episode Dis...\n",
      "89  RT @ hrw : India Needs Step Back # Kashmir | I...\n",
      "90  RT @ pradip103 : I argue removal # Article370 ...\n",
      "91  RT @ PTIofficial : India ’ seizure lockdown Ka...\n",
      "92  RT @ fred9fr : Dulu aku setuju simpan Zakir Na...\n",
      "93  RT @ TheAngryLord : # HardKaur completely lost...\n",
      "94  RT @ majorgauravarya : Yesterday Pak appealed ...\n",
      "95  RT @ cprd_india : @ Cold_Peace_ US admns time ...\n",
      "96  RT @ TheAngryLord : # HardKaur completely lost...\n",
      "97  RT @ DaaruBaazMehta : 2013 : AAP 's Promise Ch...\n",
      "98  RT @ indiatvnews : Expected : In historic move...\n",
      "99  RT @ AbhishBanerj : Merit handpicked indeed . ...\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#operation on india_tweets\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "stopwords=set(stopwords.words('english'))\n",
    "#print(stopwords)\n",
    "df=pd.read_csv(\"/Users/Apple/Desktop/india_tweets.csv\")\n",
    "#print(df)\n",
    "#stopword removal in python using nltk\n",
    "df_msg=df['Message']\n",
    "#print(df_msg)\n",
    "filtered=[]\n",
    "for i in df_msg:\n",
    "    s=\"\"\n",
    "    for word in word_tokenize(i):\n",
    "        if word not in stopwords:\n",
    "            s+=word\n",
    "            s+=\" \"\n",
    "    filtered.append(s)\n",
    "filtered_df=pd.DataFrame(filtered)\n",
    "print(\"*****************Filtered****************************\")\n",
    "print(filtered_df)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************Stemmed************************\n",
      "                                                    0\n",
      "0   RT @ vijayarahatkar : 'women love challenges.....\n",
      "1   RT @ cnn : pakistan prime minist imran khan li...\n",
      "2   jio-microsoft cloud tie-up pose threat amazon ...\n",
      "3   RT @ davidfrawleyv : pakistan defin nation inv...\n",
      "4   RT @ piyushgoy : delight address plenari 'indi...\n",
      "5   RT @ suryahsg : 1965 india - pakistan war ! ac...\n",
      "6   RT @ akhileshpsingh : articl 370 अब @ bjp4indi...\n",
      "7   RT @ piyushgoy : railway manufactur high speed...\n",
      "8   RT @ cnn : indian actress priyanka chopra crit...\n",
      "9   RT @ cnn : indian actress priyanka chopra crit...\n",
      "10  RT @ bbchindi : अमरिंदर सिंह ने यूं जीता कश्मी...\n",
      "11  RT @ iabheejit : twitter magic ! I 'm look con...\n",
      "12  RT @ amarujalanew : कश्मीर में तैनात इन दो महि...\n",
      "13  RT @ shashitharoor : interest piec @ saketwrit...\n",
      "14  RT @ puneetvizh : rbi ’ unusu rate cut add unc...\n",
      "15  RT @ nadhirthegreat : interpol even reject ind...\n",
      "16  RT @ piyushgoy : railway manufactur high speed...\n",
      "17  RT @ madhukishwar : 2 . yet hindu hate muslim ...\n",
      "18  everyth I want say said I 'm . lol liter play ...\n",
      "19  If question kashmiri journalist pakistan done ...\n",
      "20  RT @ cnnnews18 : # article370revok | peopl acr...\n",
      "21  RT @ bilalta49287662 : I alway stand kashmir s...\n",
      "22  well , I guess , confid come big ass chanc exp...\n",
      "23  RT @ asranomani : fyi @ priyankachopra “ war m...\n",
      "24  RT @ ravinarin : not forget ... crimin `` real...\n",
      "25  RT @ pramila61709169 : 46 % rape case found fa...\n",
      "26  jio-microsoft cloud tie-up pose threat amazon ...\n",
      "27  RT @ thatdoggoneladi : freelanc india , go cur...\n",
      "28  RT @ vinayak_jain : saudi arabia invest 20 % s...\n",
      "29  RT @ shefvaidya : question ask , abdul basit c...\n",
      "..                                                ...\n",
      "70  RT @ alarabiya_eng : pakistani prime minist im...\n",
      "71  RT @ jhagra : 1 . india , countri 200 mln musl...\n",
      "72  RT @ ambabsolut : @ poojabeditweet @ smritiira...\n",
      "73  RT @ trtworld : muslim india-administ kashmir ...\n",
      "74  RT @ cnn : pakistan prime minist imran khan li...\n",
      "75  RT @ inventionsved : @ serbianpm @ realdonaldt...\n",
      "76  RT @ zeenewshindi : सुषमा स्वराज को याद कर रहा...\n",
      "77  've seen pakistani troll n't behav like everi ...\n",
      "78  RT @ nytim : @ sameeryasir @ suhasiniraj the i...\n",
      "79  RT @ amritabhind : everyon world back india , ...\n",
      "80  RT @ amit_gujju : first adhir ranjan said kash...\n",
      "81  RT @ adityarajkaul : the magic moment flew pow...\n",
      "82  hey shobha De , stop send terrorist pakistan i...\n",
      "83  RT @ cmann66 : more pollut india http : //t.co...\n",
      "84  RT @ ani : # watch columnist shobhaa De respon...\n",
      "85  RT @ sharmaalok_ : lynch becom common india ’ ...\n",
      "86  RT @ indiantweet : whi would abdul basit lie s...\n",
      "87  RT @ c_aashish : illeg cow smuggl from india T...\n",
      "88  RT @ rammadhavbjp : An interest episod discove...\n",
      "89  RT @ hrw : india need step back # kashmir | in...\n",
      "90  RT @ pradip103 : I argu remov # article370 uph...\n",
      "91  RT @ ptioffici : india ’ seizur lockdown kashm...\n",
      "92  RT @ fred9fr : dulu aku setuju simpan zakir na...\n",
      "93  RT @ theangrylord : # hardkaur complet lost .....\n",
      "94  RT @ majorgauravarya : yesterday pak appeal oi...\n",
      "95  RT @ cprd_india : @ cold_peace_ US admn time t...\n",
      "96  RT @ theangrylord : # hardkaur complet lost .....\n",
      "97  RT @ daarubaazmehta : 2013 : aap 's promis che...\n",
      "98  RT @ indiatvnew : expect : In histor move , am...\n",
      "99  RT @ abhishbanerj : merit handpick inde . jawa...\n",
      "\n",
      "[100 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "ps = PorterStemmer()\n",
    "#perform stemming on the stopwords removed dataframe\n",
    "stemmed=[]\n",
    "message=filtered_df[0]\n",
    "for j in message:\n",
    "    #print(j)\n",
    "    s=\"\"\n",
    "    for word in word_tokenize(j):\n",
    "        if(isinstance(word,int)==False):\n",
    "            s+=ps.stem(word)\n",
    "            s+=\" \"\n",
    "    stemmed.append(s)\n",
    "stemmed_df=pd.DataFrame(stemmed)\n",
    "print(\"**********************Stemmed************************\")\n",
    "print(stemmed_df)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d4a6cee4672b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstemmed_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0;32m-> 1032\u001b[0;31m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[1;32m   1033\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[0;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m    326\u001b[0m                                                tokenize)\n\u001b[1;32m    327\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[0;32m--> 328\u001b[0;31m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlowercase\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrip_accents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# convertion to term document matrix\n",
    "#import textmining-1.0\n",
    "#stemmed=pd.Dataframe\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(stemmed_df)\n",
    "print(vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.toarray()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
